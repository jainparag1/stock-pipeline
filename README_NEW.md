# ğŸ“ˆ Stock Market Streaming Platform

A production-style **real-time stock market streaming and analytics platform** built to demonstrate modern data engineering, streaming systems, and lakehouse design.

This project simulates high-frequency stock ticks, processes them using Spark Structured Streaming, stores them in an S3-compatible lakehouse (MinIO), orchestrates batch compaction via Airflow, models analytics using dbt + DuckDB, and visualizes trends in real time using Streamlit.

> ğŸ§  Built to showcase **streaming expertise**, **system design**, and **tech leadership mindset**.

---

## ğŸ¯ Goals of This Project

- Demonstrate **end-to-end streaming system design**
- Showcase **production-grade Spark Structured Streaming**
- Implement **lakehouse patterns** (raw â†’ compacted â†’ analytics)
- Orchestrate batch jobs using **Airflow**
- Model analytics using **dbt**
- Visualize real-time insights with **Streamlit**
- Position the author for **Senior Engineer / Tech Lead / CTO** roles

---

## ğŸ—ï¸ High-Level Architecture

```bash
Kafka (Stock Ticks)
â†“
Spark Structured Streaming
â†“
MinIO (S3-compatible Object Storage)
â†“
Spark Compaction Jobs (Airflow Orchestrated)
â†“
dbt + DuckDB (Analytics Layer)
â†“
Streamlit Dashboard (Visualization)
```


ğŸ“„ Deep dives:
- `docs/executive-summary.md`
- `docs/failure-and-recovery.md`

---

### ğŸ“‚ Project Structure

```text
stock-pipeline/
â”‚
â”œâ”€â”€ data_simulator/        # Kafka producer simulating stock ticks
â”‚
â”œâ”€â”€ spark_processor/       # Spark streaming & compaction jobs
â”‚   â”œâ”€â”€ stream_processor.py
â”‚   â”œâ”€â”€ compaction_job.py
â”‚   â””â”€â”€ run_compaction.sh
â”‚
â”œâ”€â”€ jars/                  # Explicit Spark / Hadoop / Kafka dependencies
â”‚
â”œâ”€â”€ output/                # Raw streaming parquet output
â”œâ”€â”€ aggregates/            # Aggregated parquet outputs
â”œâ”€â”€ checkpoint/            # Spark streaming checkpoints
â”‚
â”œâ”€â”€ dashboards/            # Streamlit live dashboard
â”‚   â””â”€â”€ dashboard_live.py
â”‚
â”œâ”€â”€ airflow/               # Airflow DAGs for orchestration
â”‚   â””â”€â”€ dags/
â”‚
â”œâ”€â”€ dbt_models/            # dbt project with DuckDB
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ dev.duckdb
â”‚
â”œâ”€â”€ infra/                 # Docker / MinIO / Infra setup
â”œâ”€â”€ artifacts/             # Screenshots, diagrams, demo assets
â”œâ”€â”€ docs/                  # Architecture & design docs
â”œâ”€â”€ logs/                  # Runtime logs
â”œâ”€â”€ venv/                  # Python virtual environment
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ spark-4.0.0-bin-hadoop3.tgz
```
---

## âš™ï¸ Core Components
### ğŸš€ Data Ingestion (Kafka)

- Simulated stock tick producer
- Durable event buffering
- Decouples producers and consumers

### ğŸ”¥ Stream Processing (Spark)

- Spark Structured Streaming
- Exactly-once semantics (checkpoint-based)
- Time-partitioned Parquet output

### ğŸª£ Storage (MinIO)

- S3-compatible object storage
- Raw + compacted datasets
- Partitioned by time

### ğŸ§¹ Compaction (Spark Batch)

- Solves the small-file problem
- Rewrites partitions efficiently
- Idempotent design
- Orchestrated by Airflow

### â±ï¸ Orchestration (Airflow)

- Scheduled compaction jobs
- Partition-aware execution
- Failure retries & observability

### ğŸ“Š Analytics (dbt + DuckDB)

- SQL-based transformations
- Fast local analytics over Parquet
- Clean separation from ingestion layer

### ğŸ“ˆ Visualization (Streamlit)

- Live stock trends
- Min / Max / Avg price metrics
- Stateless, restart-friendly UI
  
---

### â–¶ï¸ Running the Platform (Local)

Please refer `docs/start-stop-services.md`

---

## ğŸ›¡ï¸ Failure Handling & Recovery

This platform is intentionally designed to:

- Isolate failures
- Prevent data loss
- Enable predictable recovery

ğŸ“„ See: docs/failure-and-recovery.md
---

### ğŸš§ Future Enhancements

- Schema Registry integration
- Delta Lake / Iceberg support
- End-to-end exactly-once guarantees
- Cloud deployment (AWS / AZURE/ GCP)
- CI/CD for Spark, Airflow, and dbt

---

### ğŸ‘¤ Author

**Parag Jain**
Director Software Engineering | Streaming & Data Platforms |
Cloud & AIML evangelist| Aspiring CTO | FinTech Systems

**ğŸ”— GitHub:** https://github.com/jainparag1**   
**ğŸ”— LinkedIn:** https://www.linkedin.com/in/parag-jain-0395011**

---

### â­ Final Note

This project is intentionally designed to reflect **real-world production tradeoffs** rather than tutorial shortcuts.

If you are a **recruiter, hiring manager, or founder,** this repository demonstrates how I approach building, operating, and scaling modern data platforms.