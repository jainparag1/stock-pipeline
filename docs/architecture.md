ğŸ—ï¸ System Architecture â€“ Stock Market Streaming Platform
1. Overview

This document describes the end-to-end architecture of the Stock Market Streaming Platform, a real-time data pipeline designed to ingest, process, store, optimize, and analyze stock market events at scale.

The system follows modern lakehouse principles and separates concerns across:

â€¢ Ingestion
â€¢ Stream processing
â€¢ Storage
â€¢ Optimization
â€¢ Analytics
â€¢ Visualization
â€¢ Orchestration

The architecture is intentionally modular to allow future migration to managed cloud services without redesign.

2. Design Principles

The platform was designed around the following principles:

ğŸ”¹ Real-Time First

â€¢ Streaming ingestion using Kafka  
â€¢ Low-latency processing via Spark Structured Streaming  
â€¢ Event-time aware processing  

ğŸ”¹ Storage as the Source of Truth

â€¢ Immutable Parquet files
â€¢ Append-only streaming writes
â€¢ Optimized for reprocessing and backfills

ğŸ”¹ Separation of Compute & Storage

â€¢ Stateless Spark jobs
â€¢ Object storage (MinIO) as durable layer
â€¢ Independent scaling of components

ğŸ”¹ Production Awareness

â€¢ Checkpointing & fault tolerance
â€¢ Compaction to solve small-file problem
â€¢ Orchestration via Airflow
â€¢ Analytics isolated from ingestion

3. High-Level Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Simulator   â”‚
â”‚ (Stock Generator)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Topics     â”‚
â”‚ stock-ticks      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Structured Streaming     â”‚
â”‚ - Schema enforcement           â”‚
â”‚ - Event-time handling          â”‚
â”‚ - Watermarking                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinIO (Object Storage)         â”‚
â”‚ - Parquet format               â”‚
â”‚ - Partitioned by time          â”‚
â”‚ - Raw streaming layer          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Compaction Job            â”‚
â”‚ - File consolidation           â”‚
â”‚ - Partition overwrite          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analytics Layer                 â”‚
â”‚ - dbt models                    â”‚
â”‚ - DuckDB query engine           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit Dashboard             â”‚
â”‚ - Live trends                   â”‚
â”‚ - Aggregates & KPIs             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Component Breakdown
4.1 Data Simulator

Purpose:
Simulates real-time stock market ticks.

Key Characteristics:

â€¢ Generates random price and volume per ticker
â€¢ Publishes JSON messages to Kafka
â€¢ Designed to mimic real-world tick velocity

Why it exists:
Allows deterministic testing and local development without external data sources.

4.2 Kafka (Ingestion Layer)

Purpose:
Acts as the durable ingestion buffer between producers and consumers.

Responsibilities:

â€¢ Decouples data producers from Spark consumers
â€¢ Handles burst traffic
â€¢ Guarantees message ordering per partition

Topics:

stock-ticks

4.3 Spark Structured Streaming (Processing Layer)

Purpose:
Processes real-time events and writes them to the data lake.

Responsibilities:

â€¢ Schema validation
â€¢ Timestamp parsing & event-time processing
â€¢ Partitioned Parquet writes
â€¢ Fault tolerance via checkpoints

Why Structured Streaming:
Provides exactly-once semantics and seamless transition between streaming and batch workloads.

4.4 Object Storage â€“ MinIO (Lakehouse Layer)

Purpose:
Acts as the central, durable storage layer.

Characteristics:

â€¢ S3-compatible API
â€¢ Stores immutable Parquet files
â€¢ Supports time-based partitioning
â€¢ Partition Strategy:
  year=YYYY/
    month=MM/
      day=DD/
        hour=HH/


This enables:

â€¢ Efficient pruning
â€¢ Time-based analytics

Scalable compaction

4.5 Compaction Job (Optimization Layer)

Problem Addressed:
Spark streaming creates many small Parquet files, which degrade query performance.

Solution:
A periodic Spark batch job that:

â€¢ Reads a specific partition
â€¢ Repartitions data
â€¢ Writes fewer, larger Parquet files
â€¢ Overwrites only the target partition

Execution:

â€¢ Triggered via Airflow DAG
â€¢ Runs independently of streaming job

4.6 Orchestration â€“ Apache Airflow

Purpose:
Coordinates batch jobs and future workflows.

Current Usage:

â€¢ Schedule compaction job every N minutes
â€¢ Monitor job execution
â€¢ Centralized operational control

Why Airflow:
Provides visibility, retries, and extensibility for future pipelines (DBT, quality checks).

4.7 Analytics Layer â€“ dbt + DuckDB

Purpose:
Transforms raw Parquet data into analytics-ready datasets.

Responsibilities:

â€¢ SQL-based transformations
â€¢ Metric computation (min, max, avg)
â€¢ Logical modeling of stock trends

Why DuckDB:

â€¢ Lightweight
â€¢ Columnar execution
â€¢ Perfect for local and embedded analytics

4.8 Visualization â€“ Streamlit

Purpose:
Provides real-time insights into processed data.

Features:

â€¢ Live price trends per ticker
â€¢ Aggregate statistics
â€¢ Auto-refreshing UI

5. Failure Handling & Reliability

â€¢ Spark checkpoints ensure recovery from failures
â€¢ Kafka offsets tracked for ingestion consistency
â€¢ Partition-level overwrite prevents global corruption
â€¢ Batch compaction isolated from streaming jobs

6. Scalability & Future Enhancements

â€¢ Replace MinIO with AWS S3 / GCS
â€¢ Scale Kafka partitions
â€¢ Deploy Spark on Kubernetes
â€¢ Introduce Trino / Athena
â€¢ Add data quality & schema evolution checks

7. Key Takeaways

This architecture demonstrates:

â€¢ Production-grade streaming design
â€¢ Lakehouse thinking
â€¢ Clear separation of concerns
â€¢ Operational maturity beyond MVPs

ğŸ§  Architectural Mindset

â€œGood data systems arenâ€™t defined by tools â€” theyâ€™re defined by boundaries, contracts, and trade-offs.â€

This project intentionally mirrors how modern data platforms are built in high-performing engineering teams.