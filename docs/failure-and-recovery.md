ðŸ”¥ Failure Scenarios & Recovery Guide

Stock Market Streaming Platform

1. Kafka Producer Failure

Scenario

â€¢ The stock data simulator crashes or stops sending events.

Impact

â€¢ No new events published
â€¢ Downstream systems remain idle
â€¢ No data loss

Detection

â€¢ Kafka topic lag stops increasing
â€¢ Spark streaming shows zero input rows/sec

Recovery

â€¢ Restart producer
â€¢ Kafka retains previous data
â€¢ Spark resumes consumption automatically

Why This Is Safe

â€¢ Kafka decouples ingestion from processing, providing natural buffering.

2. Kafka Broker Restart / Offset Issues

Scenario

â€¢ Kafka broker restarts or topic offsets reset during development.

Impact

â€¢ Potential offset mismatch
â€¢ Spark streaming job may fail

Detection

â€¢ Spark logs show offset inconsistency
â€¢ Streaming query terminates

Recovery

â€¢ Restart Spark job
â€¢ Use failOnDataLoss=false (development only)
â€¢ In production, enable Kafka retention safeguards

Trade-off

â€¢ Prefers data availability during development over strict guarantees.

3. Spark Streaming Job Crash

Scenario

â€¢ Spark process crashes due to OOM, dependency issue, or node failure.

Impact

â€¢ Streaming pauses temporarily
â€¢ No data corruption

Detection

â€¢ Spark UI shows terminated query
â€¢ No new files written

Recovery

â€¢ Restart streaming job
â€¢ Spark resumes from last checkpoint
â€¢ Exactly-once semantics preserved

Key Mechanism

â€¢ Spark checkpoints store offsets and state.

4. MinIO / Object Storage Outage

Scenario

â€¢ MinIO service becomes unavailable.

Impact

â€¢ Streaming job fails on write
â€¢ Kafka continues accumulating data

Detection

â€¢ Spark write failures
â€¢ MinIO health checks fail

Recovery

â€¢ Restart MinIO
â€¢ Restart Spark streaming job
â€¢ Backlog processed automatically

Why This Works

â€¢ Storage is downstream of Kafka â€” ingestion continues safely.

5. Small File Explosion

Scenario

â€¢ High-frequency streaming produces thousands of tiny Parquet files.

Impact

â€¢ Query performance degradation
â€¢ Increased metadata overhead

Detection

â€¢ Excessive file count per partition
â€¢ Slower DuckDB/dbt queries

Recovery

â€¢ Trigger compaction job
â€¢ Overwrite only affected partitions
â€¢ Restore optimal file sizes

6. Compaction Job Failure

Scenario

â€¢ Compaction Spark job fails mid-run.

Impact

â€¢ Partition remains unoptimized
â€¢ No data loss

Detection

â€¢ Airflow DAG failure
â€¢ Partial overwrite prevented by Spark semantics

Recovery

â€¢ Rerun compaction DAG
â€¢ Idempotent by partition

7. Airflow Scheduler Down

Scenario

â€¢ Airflow scheduler crashes.

Impact

â€¢ No scheduled compactions
â€¢ Streaming unaffected

Detection

â€¢ DAGs not triggering
â€¢ Airflow UI unavailable

Recovery

â€¢ Restart scheduler
â€¢ Manual backfill if needed

8. dbt Model Failure

Scenario

â€¢ Schema mismatch or missing partition causes dbt failure.

Impact

â€¢ Analytics models not refreshed
â€¢ Raw data remains intact

Detection

â€¢ dbt run errors
â€¢ CI or logs show failing model

Recovery

â€¢ Fix model logic
â€¢ Re-run dbt
â€¢ No impact on upstream systems

9. Dashboard Failure

Scenario

â€¢ Streamlit dashboard crashes or loses connectivity.

Impact

â€¢ Visualization unavailable
â€¢ Data pipeline continues running

Detection

â€¢ UI inaccessible

Recovery

â€¢ Restart dashboard
â€¢ Stateless design enables fast recovery

ðŸŽ¯ Design Philosophy

This system is designed so that:

â€¢ Failures are isolated
â€¢ Recovery is predictable
â€¢ No single failure causes data loss
â€¢ Data pipelines should bend, not break.

ðŸ“Œ Key Takeaway

â€¢ Most failures are operational, not architectural.
â€¢ This platform is designed to fail safely and recover cleanly.