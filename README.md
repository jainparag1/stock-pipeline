ğŸ“ˆ Stock Market Streaming Platform

Real-time Stock Data Pipeline | Kafka â€¢ Spark â€¢ MinIO â€¢ Airflow â€¢ dbt â€¢ DuckDB

A production-style, end-to-end real-time stock market data platform showcasing streaming, lakehouse architecture, and analytics engineering â€” built with scalability, observability, and orchestration in mind.

ğŸš€ Why This Project?

This project was built to demonstrate how modern data platforms actually work in the real world, not just in tutorials.

It focuses on:

âš¡ Real-time streaming with Spark Structured Streaming
ğŸ§± Lakehouse-style storage using Parquet on MinIO (S3-compatible)
ğŸ” Data compaction & optimization (often ignored, always critical)
ğŸ› ï¸ Workflow orchestration with Airflow
ğŸ“Š Analytics & modeling using dbt + DuckDB
ğŸ“º Live visualization via Streamlit

This is the kind of system youâ€™d expect in:

âœ Fintechs
âœ Trading platforms
âœ Data-driven startups
âœ Modern cloud-native data teams

ğŸ§  High-Level Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Simulatorâ”‚
â”‚ (Stock Ticks) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kafka     â”‚
â”‚ (Raw Events) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Structured Streamingâ”‚
â”‚ - Parsing & validation    â”‚
â”‚ - Event-time processing   â”‚
â”‚ - Parquet writes          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinIO (S3-compatible)     â”‚
â”‚ - Raw streaming parquet   â”‚
â”‚ - Partitioned by date     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Compaction Job      â”‚
â”‚ - File consolidation      â”‚
â”‚ - Optimized partitions    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analytics Layer           â”‚
â”‚ - dbt models              â”‚
â”‚ - DuckDB engine           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit Dashboard       â”‚
â”‚ - Live price trends       â”‚
â”‚ - Min / Max / Avg         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“‚ Project Structure (Aligned to Production Thinking)
stock-pipeline/
â”‚
â”œâ”€â”€ data_simulator/        # Kafka producer simulating stock ticks
â”‚
â”œâ”€â”€ spark_processor/       # Core Spark logic
â”‚   â”œâ”€â”€ stream_processor.py   # Structured Streaming consumer
â”‚   â”œâ”€â”€ compaction_job.py     # Parquet compaction job
â”‚   â””â”€â”€ run_compaction.sh     # Spark-submit wrapper
â”‚
â”œâ”€â”€ jars/                  # Explicit Spark / Hadoop / Kafka dependencies
â”‚
â”œâ”€â”€ output/                # Raw streaming parquet output
â”œâ”€â”€ aggregates/            # Aggregated parquet datasets
â”œâ”€â”€ checkpoint/            # Spark streaming checkpoints
â”‚
â”œâ”€â”€ dashboards/            # Streamlit live dashboard
â”‚   â””â”€â”€ dashboard_live.py
â”‚
â”œâ”€â”€ airflow/               # Airflow DAGs
â”‚   â””â”€â”€ dags/
â”‚
â”œâ”€â”€ dbt_models/            # dbt + DuckDB analytics layer
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ dev.duckdb
â”‚
â”œâ”€â”€ infra/                 # MinIO / Docker / infra setup
â”œâ”€â”€ artifacts/             # Screenshots, diagrams, demo assets
â”œâ”€â”€ docs/                  # Architecture & design notes
â”œâ”€â”€ logs/                  # Runtime logs
â”œâ”€â”€ venv/                  # Python virtual environment
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ spark-4.0.0-bin-hadoop3.tgz

âš™ï¸ Key Features
âš¡ Real-Time Streaming

â€¢ Kafka-based stock tick ingestion
â€¢ Spark Structured Streaming with event-time processing
â€¢ Fault tolerance via checkpoints

ğŸ§± Lakehouse Storage

â€¢ Parquet-based storage on MinIO (S3-compatible)
â€¢ Partitioned by year / month / day / hour
â€¢ Optimized for downstream analytics

ğŸ” Compaction & Optimization

â€¢ Periodic Spark batch job
â€¢ Reduces small-file problem
â€¢ Orchestrated via Airflow (cron-style DAG)

ğŸ“Š Analytics with dbt + DuckDB

â€¢ SQL-first transformations
â€¢ Fast local analytics without external warehouses
â€¢ Easy transition to Trino / Athena / Snowflake later

ğŸ“º Live Dashboard

â€¢ Streamlit-based UI
â€¢ Live price trends per ticker
â€¢ Min / Max / Average overlays
â€¢ Auto-refresh for near real-time insights

ğŸ› ï¸ Tech Stack
Layer	              Technology
Ingestion	       Kafka
Stream Processing	Apache Spark
Storage	       MinIO (S3-compatible)
Orchestration	       Apache Airflow
Analytics	       dbt + DuckDB
Visualization	       Streamlit
Language	       Python, SQL
Format	              Parquet

ğŸ¯ What This Project Demonstrates

â€¢ Production-aware Spark & Hadoop internals
â€¢ Real-world data engineering trade-offs
â€¢ Ability to design systems, not just write code
â€¢ In short: how a Tech Lead thinks about data platforms.

ğŸ§­ Roadmap

â€¢ Trino / Athena-style query engine
â€¢ Schema evolution handling
â€¢ Metrics & data quality checks
â€¢ Cloud deployment (AWS / GCP)
â€¢ CI/CD for data pipelines

ğŸ‘‹ About Me

Built by Parag Jain
Director of Software Engineering | Data & Streaming Enthusiast
Aiming for CTO / Tech Lead roles in high-impact startups