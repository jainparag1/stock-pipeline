ğŸ“ˆ Stock Market Streaming Platform

Real-time Stock Data Pipeline | Kafka â€¢ Spark â€¢ MinIO â€¢ Airflow â€¢ dbt â€¢ DuckDB

A production-style, end-to-end real-time stock market data platform showcasing streaming, lakehouse architecture, and analytics engineering â€” built with scalability, observability, and orchestration in mind.

ğŸš€ Why This Project?

This project was built to demonstrate how modern data platforms actually work in the real world, not just in tutorials.

It focuses on:

âš¡ Real-time streaming with Spark Structured Streaming

ğŸ§± Lakehouse-style storage using Parquet on MinIO (S3-compatible)

ğŸ” Data compaction & optimization (often ignored, always critical)

ğŸ› ï¸ Workflow orchestration with Airflow

ğŸ“Š Analytics & modeling using dbt + DuckDB

ğŸ“º Live visualization via Streamlit

This is the kind of system youâ€™d expect in:

Fintechs

Trading platforms

Data-driven startups

Modern cloud-native data teams

ğŸ§  High-Level Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Simulatorâ”‚
â”‚ (Stock Ticks) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kafka     â”‚
â”‚ (Raw Events) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Structured Streamingâ”‚
â”‚ - Parsing & validation    â”‚
â”‚ - Event-time processing   â”‚
â”‚ - Parquet writes          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinIO (S3-compatible)     â”‚
â”‚ - Raw streaming parquet   â”‚
â”‚ - Partitioned by date     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Compaction Job      â”‚
â”‚ - File consolidation      â”‚
â”‚ - Optimized partitions    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analytics Layer           â”‚
â”‚ - dbt models              â”‚
â”‚ - DuckDB engine           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit Dashboard       â”‚
â”‚ - Live price trends       â”‚
â”‚ - Min / Max / Avg         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“‚ Project Structure (Aligned to Production Thinking)
stock-pipeline/
â”‚
â”œâ”€â”€ data_simulator/        # Kafka producer simulating stock ticks
â”‚
â”œâ”€â”€ spark_processor/       # Core Spark logic
â”‚   â”œâ”€â”€ stream_processor.py   # Structured Streaming consumer
â”‚   â”œâ”€â”€ compaction_job.py     # Parquet compaction job
â”‚   â””â”€â”€ run_compaction.sh     # Spark-submit wrapper
â”‚
â”œâ”€â”€ jars/                  # Explicit Spark / Hadoop / Kafka dependencies
â”‚
â”œâ”€â”€ output/                # Raw streaming parquet output
â”œâ”€â”€ aggregates/            # Aggregated parquet datasets
â”œâ”€â”€ checkpoint/            # Spark streaming checkpoints
â”‚
â”œâ”€â”€ dashboards/            # Streamlit live dashboard
â”‚   â””â”€â”€ dashboard_live.py
â”‚
â”œâ”€â”€ airflow/               # Airflow DAGs
â”‚   â””â”€â”€ dags/
â”‚
â”œâ”€â”€ dbt_models/            # dbt + DuckDB analytics layer
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ dev.duckdb
â”‚
â”œâ”€â”€ infra/                 # MinIO / Docker / infra setup
â”œâ”€â”€ artifacts/             # Screenshots, diagrams, demo assets
â”œâ”€â”€ docs/                  # Architecture & design notes
â”œâ”€â”€ logs/                  # Runtime logs
â”œâ”€â”€ venv/                  # Python virtual environment
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ spark-4.0.0-bin-hadoop3.tgz

âš™ï¸ Key Features
âš¡ Real-Time Streaming

Kafka-based stock tick ingestion

Spark Structured Streaming with event-time processing

Fault tolerance via checkpoints

ğŸ§± Lakehouse Storage

Parquet-based storage on MinIO (S3-compatible)

Partitioned by year / month / day / hour

Optimized for downstream analytics

ğŸ” Compaction & Optimization

Periodic Spark batch job

Reduces small-file problem

Orchestrated via Airflow (cron-style DAG)

ğŸ“Š Analytics with dbt + DuckDB

SQL-first transformations

Fast local analytics without external warehouses

Easy transition to Trino / Athena / Snowflake later

ğŸ“º Live Dashboard

Streamlit-based UI

Live price trends per ticker

Min / Max / Average overlays

Auto-refresh for near real-time insights

ğŸ› ï¸ Tech Stack
Layer	Technology
Ingestion	Kafka
Stream Processing	Apache Spark
Storage	MinIO (S3-compatible)
Orchestration	Apache Airflow
Analytics	dbt + DuckDB
Visualization	Streamlit
Language	Python, SQL
Format	Parquet
ğŸ¯ What This Project Demonstrates

End-to-end ownership mindset

Production-aware Spark & Hadoop internals

Real-world data engineering trade-offs

Ability to design systems, not just write code

In short: how a Tech Lead thinks about data platforms.

ğŸ§­ Roadmap

 Trino / Athena-style query engine

 Schema evolution handling

 Metrics & data quality checks

 Cloud deployment (AWS / GCP)

 CI/CD for data pipelines

ğŸ‘‹ About Me

Built by Parag Jain
Senior Software Engineer | Data & Streaming Enthusiast
Aiming for CTO / Tech Lead roles in high-impact startups